{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01935c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model comparison results with ranking have been saved to model_comparison_results_with_ranking.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "data = pd.read_csv('ABQ_2017_dni.csv')\n",
    "\n",
    "# Handle NaN, infinity, or large values in the dataset\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Extract the reference column (dni)\n",
    "reference = data['dni'].values\n",
    "\n",
    "# Extract the prediction columns\n",
    "prediction_columns = ['dni_erbs', 'dni_erbsD', 'dni_ohm', 'dni_louche', 'dni_boland',  'dni_Engerer2','dni_Engerer3', 'dni_Engerer4',\n",
    "                      'dni_Yang5', 'dni_Yang4', 'dni_JKT', 'dni_Mabasa'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'rMBE': [],\n",
    "    'MBE': [],\n",
    "    'MAE': [],\n",
    "    'rMAE': [],\n",
    "    'RMSE': [],\n",
    "    'rRMSE': [],\n",
    "    'R2': []\n",
    "}\n",
    "\n",
    "# Function to calculate MBE\n",
    "def mean_bias_error(y_true, y_pred):\n",
    "    return np.mean(y_pred - y_true)\n",
    "\n",
    "# Iterate over each prediction column and calculate the metrics\n",
    "for col in prediction_columns:\n",
    "    predictions = data[col].values\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mbe = mean_bias_error(reference, predictions)\n",
    "    mae = mean_absolute_error(reference, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(reference, predictions))\n",
    "    r2 = r2_score(reference, predictions)\n",
    "    \n",
    "    # Calculate relative metrics\n",
    "    rMBE = mbe / np.mean(reference) * 100\n",
    "    rMAE = mae / np.mean(reference) * 100\n",
    "    rRMSE = rmse / np.mean(reference) * 100\n",
    "    \n",
    "    # Append results to the dictionary\n",
    "    results['Model'].append(col)\n",
    "    results['rMBE'].append(rMBE)\n",
    "    results['MBE'].append(mbe)\n",
    "    results['MAE'].append(mae)\n",
    "    results['rMAE'].append(rMAE)\n",
    "    results['RMSE'].append(rmse)\n",
    "    results['rRMSE'].append(rRMSE)\n",
    "    results['R2'].append(r2)\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round off the results to 3 decimal digits\n",
    "results_df = results_df.round(3)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('ABQ2_dni_metrics_results.csv', index=False)\n",
    "\n",
    "# Calculate absolute value of MBE\n",
    "results_df['abs_MBE'] = results_df['MBE'].abs()\n",
    "\n",
    "# Perform mean linear ranking using absolute MBE, MAE, and RMSE\n",
    "results_df['rank_abs_MBE'] = results_df['abs_MBE'].rank()\n",
    "results_df['rank_MAE'] = results_df['MAE'].rank()\n",
    "results_df['rank_RMSE'] = results_df['RMSE'].rank()\n",
    "\n",
    "# Calculate mean rank\n",
    "results_df['mean_rank'] = results_df[['rank_abs_MBE', 'rank_MAE', 'rank_RMSE']].mean(axis=1)\n",
    "\n",
    "# Round off the results to 2 decimal digits\n",
    "results_df = results_df.round(2)\n",
    "\n",
    "# Add a numeric rank column based on mean_rank starting from 1\n",
    "results_df['Numeric_Rank'] = results_df['mean_rank'].rank(method='min').astype(int)\n",
    "\n",
    "# Save the final results to a CSV file\n",
    "results_df.to_csv('ABQ2_dni_model_comparison_results_with_ranking.csv', index=False)\n",
    "\n",
    "print(\"The model comparison results with ranking have been saved to model_comparison_results_with_ranking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982599e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
